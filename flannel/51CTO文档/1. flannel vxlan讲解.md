## 1. vxlan介绍

   虚拟可扩展LAN (VXLAN)是一种网络虚拟化技术，它试图解决与大型云计算部署相关的可伸缩性问题。它使用一种类似于vlan的封装技术，将OSI的第2层以太网帧封装在第4层UDP数据报中，使用4789（windows上），linux上当前默认的是8472端口，作为默认的IANA分配的目标UDP端口号。终止VXLAN隧道的VXLAN端点可以是虚拟的或物理的交换端口，它们被称为VXLAN隧道端点(VTEPs)。

VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里

而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。

而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。

```shell
[root@node1 deploy]# k get po -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE    NOMINATED NODE   READINESS GATES
nginx-69455ff7f7-vmv7x   1/1     Running   0          25m   10.244.2.5   node3   <none>           <none>
nginx-69455ff7f7-vwfm2   1/1     Running   0          26m   10.244.1.4   node2   <none>           <none>
```

访问流程如下：
1. 容器1的IP10.244.1.4去访问容器2的IP10.244.2.5，它首先会经过cni0网桥，然后再路由到flannel.1设备进行处理。

2. 为了能够将容器1的发送到正确的主机，那么VXLAN就需要找这条通道的出口，即目的主机的VETP设备。
```shell
[root@node2 ~]# ip a 
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:16:3e:18:c2:35 brd ff:ff:ff:ff:ff:ff
    inet 172.16.195.60/20 brd 172.16.207.255 scope global dynamic eth0
       valid_lft 315354081sec preferred_lft 315354081sec
    inet6 fe80::216:3eff:fe18:c235/64 scope link 
       valid_lft forever preferred_lft forever
5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default 
    link/ether a6:69:99:7a:05:53 brd ff:ff:ff:ff:ff:ff
    inet 10.244.1.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::a469:99ff:fe7a:553/64 scope link 
       valid_lft forever preferred_lft forever
6: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether 22:57:07:3c:45:ef brd ff:ff:ff:ff:ff:ff
    inet 10.244.1.1/24 brd 10.244.1.255 scope global cni0
       valid_lft forever preferred_lft forever
    inet6 fe80::2057:7ff:fe3c:45ef/64 scope link 
       valid_lft forever preferred_lft forever
```
```shell
[root@node2 ~]# ip route 
default via 172.16.207.253 dev eth0 
10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink 
10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1 
10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink
```
从上面的路由可以看出，肯定是匹配第三条的，指明应该由flannel.1设备发出，下一跳地址是10.244.2.0，而此IP正好是node3的flannel.1地址。
```shell
[root@node3 ~]# ip a
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:16:3e:11:cf:0d brd ff:ff:ff:ff:ff:ff
    inet 172.16.195.59/20 brd 172.16.207.255 scope global dynamic eth0
       valid_lft 315353927sec preferred_lft 315353927sec
    inet6 fe80::216:3eff:fe11:cf0d/64 scope link 
       valid_lft forever preferred_lft forever
5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default 
    link/ether 22:0d:90:0d:82:c2 brd ff:ff:ff:ff:ff:ff
    inet 10.244.2.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::200d:90ff:fe0d:82c2/64 scope link 
       valid_lft forever preferred_lft forever
6: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether 02:60:55:9e:a6:78 brd ff:ff:ff:ff:ff:ff
    inet 10.244.2.1/24 brd 10.244.2.255 scope global cni0
       valid_lft forever preferred_lft forever
    inet6 fe80::60:55ff:fe9e:a678/64 scope link 
       valid_lft forever preferred_lft forever
```
可node2怎么知道10.244.2.0/32的mac地址呢？
如下可以看到
```shell
[root@node2 ~]# ip neigh show dev flannel.1
10.244.0.0 lladdr 0e:1d:a3:71:1e:4a PERMANENT
10.244.2.0 lladdr 22:0d:90:0d:82:c2 PERMANENT
```
Linux网桥是网桥的软件实现，这是Linux内核的内核部分。与硬件网桥相类似，Linux网桥维护了一个2层转发表（也称为MAC学习表，转发数据库，或者仅仅称为FDB），它跟踪记录了MAC地址与端口的对应关系。当一个网桥在端口N收到一个包时（源MAC地址为X），它在FDB中记录为MAC地址X可以从端口N到达。这样的话，以后当网桥需要转发一个包到地址X时，它就可以从FDB查询知道转发到哪里。构建一个FDB常常称之为“MAC学习”或仅仅称为“学习”过程。
```shell
[root@node2 ~]# bridge fdb show dev flannel.1
0e:1d:a3:71:1e:4a dst 172.16.195.61 self permanent
22:0d:90:0d:82:c2 dst 172.16.195.59 self permanent
```

此时，内部的IP包和数据帧就可以封装了。



#### 2. 实验
由于默认就是采用的vxlan模式，所以在这里直接查看。


1. 安装抓包工具，查看节点绑定的网桥

```shell
$ yum -y install bridge-utils
$ yum -y install tcpdump
$ brctl show cni0
bridge name	bridge id		STP enabled	interfaces
cni0		8000.3a8e5d2b5d5f	no		vethb4e68784
							vethfd756583
```

3. ping 测试

```shell
[root@c720111 ~]# kubectl exec web-8wb52 -it -- /bin/sh
# ping 10.244.2.69
PING 10.244.2.69 (10.244.2.69): 48 data bytes
56 bytes from 10.244.2.69: icmp_seq=0 ttl=62 time=2.836 ms
56 bytes from 10.244.2.69: icmp_seq=1 ttl=62 time=0.758 ms
```

4. 抓包测试

```shell
# 在cni0网桥抓包
[root@c720112 ~]# tcpdump -i cni0 -nn icmp

# 在flannel.1上抓包
[root@c720112 ~]# tcpdump -i flannel.1 -nn icmp

# 在eth0上抓包
[root@c720112 ~]# tcpdump -i eth0 -nn host 192.168.60.87
```
